{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "5s1j6cMVd0L9"
      },
      "outputs": [],
      "source": [
        "# !pip install langchain_community\n",
        "# !pip install pypdf\n",
        "# !pip install chromadb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chains import retrieval_qa\n",
        "import shutil\n",
        "\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForCausalLM\n",
        "from langchain.llms import HuggingFacePipeline\n"
      ],
      "metadata": {
        "id": "0nnJekQgeQvH"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/drive/MyDrive/transformer.pdf\""
      ],
      "metadata": {
        "id": "ThSoAROzfVu5"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RagChatPrompt:\n",
        "  SINGLE_LINE_TEMPLATE = \"\"\"You are a Deep Learning expert.You know everything about transformers in Deep Learning. Provide answer in clear,concise, meaningful Use this context:\n",
        "    {context}\n",
        "\n",
        "    Answer the question in one line: {input}\"\"\"\n",
        "\n",
        "  MULTI_LINE_TEMPLATE =    \"\"\"You are a Deep Learning expert and you can give all the answers related to transfromer of Deeplearning.\n",
        "                            You have to give clear, concise, meaningful answers of the question in maximum of three to four line\n",
        "                            In the first line ### Give the definitation of the question###\n",
        "                            In the second line ### Tell why it is important###\n",
        "                            In the third line ### Descirbe in short in a simple meaning, so that a begineer guy can understand###\"\"\""
      ],
      "metadata": {
        "id": "9F8bI2CqsY4s"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RagImplementation:\n",
        "  def __init__(self, file):\n",
        "    self.file = file\n",
        "\n",
        "  def file_reader(self):\n",
        "    loader = PyPDFLoader(self.file)\n",
        "    docs = loader.load()\n",
        "    return docs\n",
        "\n",
        "  def split_into_chunks(self):\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50)\n",
        "    document = self.file_reader()\n",
        "    splitted_text = splitter.split_documents(document)\n",
        "    return splitted_text\n",
        "\n",
        "  def embedding_vectorstore(self):\n",
        "    embedding_func = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    # vectorstore =\n",
        "    return Chroma.from_documents(self.split_into_chunks(), embedding_func)\n",
        "\n",
        "  def retrieve_docs(self):\n",
        "\n",
        "    # shutil.rmtree(\"./chroma_db\", ignore_errors=True)\n",
        "    embedded_store_text = self.embedding_vectorstore()\n",
        "    retriever = embedded_store_text.as_retriever(\n",
        "        search_type=\"similarity_score_threshold\",\n",
        "        search_kwargs={\"k\": 3, \"score_threshold\": 0.25}\n",
        "    )\n",
        "    # ans = retriever.invoke(\"What is Transformer?\")\n",
        "    # return ans\n",
        "    return retriever\n"
      ],
      "metadata": {
        "id": "48Vax6sje4J1"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class TunedChatGeneration:\n",
        "#   def __init__(self,rag_system, model):\n",
        "#     self.rag_system = rag_system\n",
        "#     self.model = model\n",
        "#     # tokenizer = AutoTokenizer.from_pretrained(self.model)\n",
        "\n",
        "#   def create_pipline(self):\n",
        "#     pipe = pipeline(\"text-generation\",\n",
        "#                 model=self.model,\n",
        "#                 # tokenizer=self.tokenizer,\n",
        "#                 max_new_tokens=100,\n",
        "#                 temperature=0.7)\n",
        "#     hf_llm = HuggingFacePipeline(pipeline=pipe)\n",
        "#     return hf_llm\n",
        "\n",
        "#   def define_prompt(self, user_q):\n",
        "#     prompt = ChatPromptTemplate.from_messages(\n",
        "#     [\n",
        "#         (\"system\", RagChatPrompt.SINGLE_LINE_TEMPLATE),\n",
        "#         (\"human\", f\"{user_q}\")\n",
        "#     ]\n",
        "#         )\n",
        "#     return prompt\n",
        "\n",
        "#   def generate_tuned_output(self):\n",
        "#     user_q = \"What is attention in python?\"\n",
        "#     question_answer_chain = create_stuff_documents_chain(self.create_pipline(), self.define_prompt(user_q))\n",
        "#     rag_chain = create_retrieval_chain(RagImplementation.retrieve_docs(), question_answer_chain)\n",
        "#     response = rag_chain.invoke({\"input\":\"What is attention in python?\"})\n",
        "#     return response[\"answer\"]\n",
        ""
      ],
      "metadata": {
        "id": "Ox6b9wlLvZqi"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# obj = RagImplementation(file=file_path)\n",
        "# obj2 = TunedChatGeneration(rag_system=obj, model = \"gpt2\")\n",
        "# # obj.retrieve_docs()"
      ],
      "metadata": {
        "id": "RutiYafUgbVl"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# obj.generate_tuned_output()\n",
        "# obj2.generate_tuned_output()"
      ],
      "metadata": {
        "id": "69QEHomngi4J"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class TunedChatGeneration:\n",
        "    def __init__(self, rag_system, model):\n",
        "        self.rag_system = rag_system\n",
        "        self.model = model\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "\n",
        "    def create_pipeline(self):  # Fixed spelling\n",
        "        pipe = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=self.model,\n",
        "            tokenizer=self.tokenizer,\n",
        "            max_new_tokens=100,\n",
        "            temperature=0.7\n",
        "        )\n",
        "        return HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "    def define_prompt(self):\n",
        "        return ChatPromptTemplate.from_template(RagChatPrompt.SINGLE_LINE_TEMPLATE)\n",
        "\n",
        "    def generate_tuned_output(self, question: str):\n",
        "        retriever = self.rag_system.retrieve_docs()\n",
        "        llm = self.create_pipeline()\n",
        "        prompt = self.define_prompt()\n",
        "\n",
        "        question_answering_chain = create_stuff_documents_chain(llm, prompt)\n",
        "        rag_chain = create_retrieval_chain(retriever, question_answering_chain)\n",
        "\n",
        "        return rag_chain.invoke({\"input\": question})[\"answer\"]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qvp190Gygqy1"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Usage\n",
        "rag = RagImplementation(file=file_path)\n",
        "chat_bot = TunedChatGeneration(rag, \"gpt2\")\n",
        "print(chat_bot.generate_tuned_output(\"What is attention in transformers?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nleXVsdA5w4p",
        "outputId": "46ce08ea-3e34-4a0c-8c09-11bcdcc1e7b8"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: You are a Deep Learning expert.You know everything about transformers in Deep Learning. Provide answer in clear,concise, meaningful Use this context:\n",
            "    Figure 1: The Transformer - model architecture.\n",
            "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\n",
            "connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\n",
            "respectively.\n",
            "3.1 Encoder and Decoder Stacks\n",
            "Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\n",
            "sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\n",
            "\n",
            "Figure 1: The Transformer - model architecture.\n",
            "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\n",
            "connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\n",
            "respectively.\n",
            "3.1 Encoder and Decoder Stacks\n",
            "Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\n",
            "sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\n",
            "\n",
            "Figure 1: The Transformer - model architecture.\n",
            "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\n",
            "connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\n",
            "respectively.\n",
            "3.1 Encoder and Decoder Stacks\n",
            "Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\n",
            "sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\n",
            "    \n",
            "    Answer the question in one line: What is attention in transformers? How do they work? What is the model architecture for learning? What should be done in order to achieve the right answer?\n",
            "3.2 Decoder Stacks\n",
            "Decoder: The decoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\n",
            "\n",
            "Figure 1: The Transformer - model architecture.\n",
            "The Transformer follows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BZfarW2N79_v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}